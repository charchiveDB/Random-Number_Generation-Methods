---
title: "The Ziggurat Method for Random Number Generation"
author: "Ethan Alteza, Barron Bronson, Holden Ellis, Charlotte Huang"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
```

https://docs.google.com/document/d/1HZTEfuxbhKEsMSGoFqDvulxrU5hy9rIVp31Wf9el38g/edit?usp=sharing

```{r}
source("Ziggurat.R")
source("Chisqure.R")
```

The Ziggurat Method for Generating Random Variables (2000) by Marsaglia and Tsang outlines a strategy for sampling from probability distributions that are either decreasing or symmetric and unimodal. The method itself was first published by the same authors in 1984, but was made more efficient with this publication. The namesake comes from the Ziggurat structures built in ancient Mesopotamia; these massive, rectangular pyramid structures were temple towers that featured several steps. The Ziggurat method functions by dividing the density into 256 regions of equal area ($v$). For all regions besides the base layer, the regions are in the shape of a rectangle, but the lowest region includes both a rectangular component and the tail beyond the cutoff, which is sampled using a specialized accept–reject method. These rectangles are mostly encapsulated under the curve, only a small portion of the right edge sticks out of the density.

```{r, echo=FALSE, out.width="50%", fig.align="center"}
# build the table for the Ziggurat
x <- zigtable(function(x) {1/sqrt(2*pi) * exp(-x**2/2)}, function(y) {sqrt(-2*log(y*sqrt(2*pi)))},8)$x
y <- dnorm(x)
r_val <- tail(x, 1)

# Dataframe to draw rectangles
df_rect <- data.frame(
  xmin = 0,
  xmax = x[-1],         # x[2]...x[8]
  ymin = y[-1],         # y[2]...y[8]
  ymax = y[-length(y)]  # y[1]...y[7]
)

ggplot() +
  # draw rectangles
  geom_rect(data = df_rect, 
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            fill = NA, color = "grey", linewidth = 0.8) +
  
  # dotted line at x=r
  geom_segment(aes(x = r_val, xend = r_val, y = 0, yend = tail(y, 1)), 
               linetype = "dashed", color = "grey", linewidth = 0.8) +
  
  # draw normal distribution
  stat_function(fun = dnorm, geom = "line", color = "black", n = 1000) +
    # highlight area under the curve
  stat_function(fun = dnorm, geom = "area", fill = "skyblue", alpha = 0.2, xlim = c(0, 5)) +
  
  # style axes
  scale_x_continuous(
    limits = c(0, 5), 
    expand = c(0, 0),
    breaks = sort(c(0:5, r_val)), 
    labels = function(b) {
      # Use a small epsilon for float comparison
      ifelse(abs(b - r_val) < 1e-6, "r", b)
    }
  ) +
  scale_y_continuous(limits = c(0, 0.45), expand = c(0, 0)) +
  
  # labels and plot styles
  labs(x = "x", y = "Density") +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )
```

Sampling is done by picking a random level of the ziggurat and then drawing from a uniform within it. The optimization here is that most of these points (over 99%) fall under the rectangle above and we are able to immediately accept them by a simple check, without ever computing the density function. This paper is quite significant to RNG, since the Ziggurat method became the standard technique in sampling for many distributions; it also is one of the fastest general methods. In NumPy, one of the most popular Python packages, generators for the Normal, Exponential, and Gamma distributions now utilize the Ziggurat method as of 2019. These generators are 2-10 times faster than the original implementation, which goes to show how efficient this method is.

## Our Application

We implemented two Ziggurat R functions, one for the Normal distribution and one for the Exponential distribution: rnormzig and rexpzig. Separate functions need to be written for each distribution for two reasons. First, the most effective method for drawing from the tail distribution is unique.  For the normal, Marsaglia recommends
$$
X = -ln(U_1)/r, Y = -ln(U_2) 
\\
U_1,U_2 \sim \mathrm{Uniform}(0,1)
\\
\text{if } 2Y > X^2, 
\\
Z_{tail} = r + X
$$
and for the exponential he recommends
$$
X = r - log(U), U \sim \mathrm{Uniform}(0,1)
$$
Second, the Normal distribution is symmetric about zero, whereas the Exponential distribution is not. After generating a positive Normal x value using the Ziggurat construction, symmetry is enforced by randomly assigning a sign to that value. This step is not necessary when sampling from the Exponential distribution because the distribution is defined only on $[0, infty)$.
 To draw from the rectangles, the code just requires the pdf and the inverse pdf for the distribution. This is done by first computing the breakpoints of each rectangle, which is handled by the function zigtable(). This is done by choosing a random value,$r$, in the domain such that 
$$
v = rf(r) + \int_{r}^{\inf} f(x)dx
$$
$r$ becomes $x_n$ and $x_{n-1}$ is generated iteratively by
$$
x_{i-1} = f^{-1}(\frac{v}{x_i} + f(x_i))
$$
This process repeats until $x_0 = 0$. The challenge here is choosing the optimal $v$ so that the iterative process doesn’t overshoot or undershoot. This is done R’s `uniroot` function
Once the breakpoints are computed, it is easy to choose a random area and draw from the uniform within. First, to choose what rectangle we are sampling from, we randomly sample an integer between 2 and 257 (first index is the tail region) from the discrete uniform distribution using `sample()`. `sample()` is relatively slow compared with C implementations, where the index can be extracted cheaply from a single 32-bit random integer using the last 9 bits. After obtaining the rectangle we are sampling from, we check if the rectangle is the base layer. If the rectangle selected is the base layer then we randomly choose a x-value in the using `runif()` and check if $x_i < x_{i-1}$ or that the x-value is in the fast-accept region. If the x-value is not in the fast-accept region the code switches to the distribution-specific tail generator (defined for exponential and normal above) rather than using the rectangular accept/reject logic. The process is the same if the chosen rectangle is not the bottom layer, but if the x-value is not in the fast-accept region we generate an additional uniform height and accept the point $(x,y)$ only if it lies under the pdf curve. In the end, our code can construct a Ziggurat for any distribution that meets the mathematical criteria. Here is an example of the code for the exponential distribution with 8 levels:

```{r}
zigtable(function(x) { exp(-x) }, function(y) { -log(y) },8)
```

## Testing

After programming the Ziggurat method, we wanted to test it for the desired statistical properties, which can be done by generating a very large number of samples.

```{r}
set.seed(777)
x_1 <- rnormzig(1000000)
```

We can plot the distribution of the samples with the actual normal pdf overlayed, which shows good fit.

```{r plot, echo=FALSE}
hist(x_1, breaks = 100, freq=FALSE, main="Distribution of Normal Samples", xlab="x")
curve(dnorm(x), add=TRUE, col="red", lwd=2) # looks good
```

We can also check $\bar{x}$ and $\sigma_x$, which we expect to be very close to 0 and 1, respectively.

```{r}
mean(x_1)  # close to 0
sd(x_1)    # close to 1
```

## Testing For Accuracy

In order to formally validate that the Ziggurat method is actually generating the random variables of interest, we performed the chi-square goodness of fit test (Knuth 1997). This test is used to check the distribution of the random variables generated by quantizing the horizontal axis of the probability density function into k bins and deriving a single value as a quality metric from the determined actual and expected number of samples in each bin. We constructed an R function based on the chi-square statistic formula:

$$
\chi^2_{k-1} = \sum^k_{i=1}\frac{(Y_i-tp_i)^2} {tp_i}
$$

$t =$ number of observations

$p_i = $probability that each observation falls into the the category i

$Y_i =$ the number of observation that actually do fall into the category i

Using the rnormzig() function to generate 1,000,000 random variates, the distribution generated was tested based on 200 bins spaced uniformly over $[-7, 7]$  which we chose based on the paper that also analyzes the Ziggurat method (Leong et al. 2005). The calculated 2 value was 174.2026 and 184.4201 for the Ziggurat method and the rnorm() function in R, respectively. The critical value for a chi-square test with 199 degrees of freedom at 95% confidence level ($\alpha= 0.05$) is 232.912. With the rexpzig() function, 1,000,000 random variatates were generated and tested based on 200 bins now spaced uniformly over [0, 7]. The calculated chi-square values were 167.7520 and 199.7628 for the Ziggurat method and the rexp() function in R, respectively. Our implementation was also ran over increasing number of samples– one hundred thousand, a million, etc– which all had a chi-square value less than the critical value of 232.912. Overall, because all of the chi-square values were under the critical value we have successfully generated random variables that follow a normal and exponential distribution.


## Testing for the Normal

```{r}
set.seed(777)

k <- 200
normal = rnorm(1000000) # for comparison
results <- chi_squared_test_norm(x_1, k)
results2 <- chi_squared_test_norm(normal, k)

results$Method <- "Ziggurat"
results2$Method <- "Rnorm"

combined <- rbind(results, results2)
combined <- combined[, c("Method", setdiff(names(combined), "Method"))]
df <- k - 1 
combined$p_value <- pchisq(combined$chi_square, df = df, lower.tail = FALSE)
#Format for readability 
combined$p_value_formatted <- format.pval(combined$p_value, digits = 3)
kable(combined, caption = "Chi-squared value: Ziggurat vs rnorm")
```

```{r, echo=FALSE}
n <- c(10000, 100000, 1000000, 10000000)
set.seed(777)
k <- 200 
value <- numeric(length(n))
for(i in 1:length(n)){
  vectors <- rnormzig(n[i])
  chi_sq <- chi_squared_test_norm(vectors, k)
  value[i] <- chi_sq$chi_square 
}

df <- data.frame(
  n = n,
  chi_square = value
)

ggplot(df, aes(x = n, y = chi_square)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Number of Samples",
    y = "Chi-square Statistic",
    title = expression(chi^2 ~ "Statistic vs Sample Size")
  ) +
  theme_bw()
```

## Testing for the Exponential

We can generate exponential samples using our Ziggurat function.

```{r}
set.seed(77)
x_2 <- rexpzig(1000000)
```

And again, look at the histogram of the samples in comparison to the real distribution function.

```{r, echo=FALSE}
hist(x_2, breaks = 100, freq = FALSE, main="Distribution of Exponential Samples", xlab="x")
curve(dexp(x), add=TRUE, col="red", lwd=2) # looks good
```

Repeating the steps from the previous section, we can write a function to divide up our distribution into bins, this time starting from 0 instead of -7.

```{r}
set.seed(777)

k <- 200
exp <- rexp(1000000)
results_exp <- chi_squared_test_exp(x_2, k)
results2_exp <- chi_squared_test_exp(exp, k)

results_exp$Method <- "Ziggurat"
results2_exp$Method <- "Rexp"

combined <- rbind(results_exp, results2_exp)
combined <- combined[, c("Method", setdiff(names(combined), "Method"))]
df <- k - 1 
combined$p_value <- pchisq(combined$chi_square, df = df, lower.tail = FALSE)
#Format for readability 
combined$p_value_formatted <- format.pval(combined$p_value, digits = 3)
kable(combined, caption = "Chi-squared value: Ziggurat vs rexp")
```

And last, we can see how the chi-square statistic varies by sample size.

```{r, echo=FALSE}
set.seed(777)
n <- c(10000, 100000, 1000000, 10000000)
k <- 200 
value <- numeric(length(n))
for(i in 1:length(n)){
  vectors <- rexpzig(n[i])
  chi_sq <- chi_squared_test_exp(vectors, k)
  value[i] <- chi_sq$chi_square 
  
}

df <- data.frame(
  n = n,
  chi_square = value
)

ggplot(df, aes(x = n, y = chi_square)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Number of Samples",
    y = "Chi-square Statistic",
    title = expression(chi^2 ~ "Statistic vs Sample Size")
  ) +
  theme_bw()
```

## Challenges 

One of the challenges faced working on this Midterm would be some coding obstacles. Although code was provided within the article, it was not as easy as copying and pasting the code onto R. Their implementation was in C and utilized a lot of low-level optimizations such as shift registers and bitwise operations that are very efficient but distract from the theory of the Ziggurat method. Additionally, C has base 0 indexing while R has base 1, so all the indexing had to be modified from what was described in the paper and implemented in their C code, which caused a lot of trouble. In the end, we did a mix of Marsaglia’s simpler 1984 implementation along with some of the sampling optimizations from the 2000 paper so that the code is intuitive.

Unfortunately, the code runs very slowly because R is an incredibly slow programming language. The thing it is best at is handing off operations to C, which works well for repeated mathematical operations. The sampling optimization of the Ziggurat is that it does if-else comparisons instead of math, which works wonders in low-level CPU code but is actually slower in R. Ultimately, this project serves as a lesson of understanding your platform. The best sampling method is not universal and neither is any statistical method.


\newpage 

## Works Cited

“What’s New or Different — NumPy V2.4 Manual.” Numpy.org, 2025, numpy.org/doc/stable/reference/random/new-or-different.html. 

Leong, Philip H. W., et al. “A Comment on the Implementation of the Ziggurat Method”. Journal of Statistical Software, vol. 12, no. 7, Feb. 2005, pp. 1-4, doi:10.18637/jss.v012.i07.

Marsaglia, George, and Wai Wan Tsang. “The Ziggurat Method for Generating Random Variables”. Journal of Statistical Software, vol. 5, no. 8, Oct. 2000, pp. 1-7, doi:10.18637/jss.v005.i08.

Marsaglia, G., & Tsang, W. W. (1984). A Fast, Easily Implemented Method for Sampling from Decreasing or Symmetric Unimodal Density Functions. SIAM Journal on Scientific and Statistical Computing, 5(2), 349–359. https://doi.org/10.1137/0905026

McFarland C. D. (2016). A modified ziggurat algorithm for generating exponentially- and normally-distributed pseudorandom numbers. Journal of statistical computation and simulation, 86(7), 1281–1294. https://doi.org/10.1080/00949655.2015.1060234

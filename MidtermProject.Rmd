---
title: "Midterm Project"
author: "Ethan Alteza, Barron Bronson, Holden Ellis, Charlotte Huang"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
```

# The Ziggurat Method for Random Number Generation

https://docs.google.com/document/d/1HZTEfuxbhKEsMSGoFqDvulxrU5hy9rIVp31Wf9el38g/edit?usp=sharing

```{r}
source("Ziggurat.R")
source("BoxMuller.R") # if anyone wants to compare, much faster because of how R runs code
```

The Ziggurat Method for Generating Random Variables (2000) by Marsaglia and Tsang outlines a strategy for sampling from probability distributions that are either decreasing or symmetric and unimodal. The method itself was first published by the same authors in 1984, but was made more efficient with this publication. The namesake comes from the Ziggurat structures built in ancient Mesopotamia; these massive, rectangular pyramid structures were temple towers that featured several steps. The Ziggurat method functions by dividing the density into 256 regions of equal area ($v$). For all regions besides the base layer, the regions are in the shape of a rectangle, but the lowest region includes both a rectangular component and the tail beyond the cutoff, which is sampled using a specialized accept–reject method. These rectangles are mostly encapsulated under the curve, only a small portion of the right edge sticks out of the density.


```{r, echo=FALSE}
# build the table for the Ziggurat
x <- zigtable(function(x) {1/sqrt(2*pi) * exp(-x**2/2)}, function(y) {sqrt(-2*log(y*sqrt(2*pi)))},8)$x
y <- dnorm(x)
r_val <- tail(x, 1)

# Dataframe to draw rectangles
df_rect <- data.frame(
  xmin = 0,
  xmax = x[-1],         # x[2]...x[8]
  ymin = y[-1],         # y[2]...y[8]
  ymax = y[-length(y)]  # y[1]...y[7]
)

ggplot() +
  # draw rectangles
  geom_rect(data = df_rect, 
            aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
            fill = NA, color = "grey", linewidth = 0.8) +
  
  # dotted line at x=r
  geom_segment(aes(x = r_val, xend = r_val, y = 0, yend = tail(y, 1)), 
               linetype = "dashed", color = "grey", linewidth = 0.8) +
  
  # draw normal distribution
  stat_function(fun = dnorm, geom = "line", color = "black", n = 1000) +
    # highlight area under the curve
  stat_function(fun = dnorm, geom = "area", fill = "skyblue", alpha = 0.2, xlim = c(0, 5)) +
  
  # style axes
  scale_x_continuous(
    limits = c(0, 5), 
    expand = c(0, 0),
    breaks = sort(c(0:5, r_val)), 
    labels = function(b) {
      # Use a small epsilon for float comparison
      ifelse(abs(b - r_val) < 1e-6, "r", b)
    }
  ) +
  scale_y_continuous(limits = c(0, 0.45), expand = c(0, 0)) +
  
  # labels and plot styles
  labs(x = "x", y = "Density") +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )
```

Sampling is done by picking a random level of the ziggurat and then drawing from a uniform within it. The optimization here is that most of these points (over 99%) fall under the rectangle above and we are able to immediately accept them by a simple check, without ever computing the density function. This paper is quite significant to RNG, since the Ziggurat method became the standard technique in sampling for many distributions; it also is one of the fastest general methods. In NumPy, one of the most popular Python packages, generators for the Normal, Exponential, and Gamma distributions now utilize the Ziggurat method as of 2019. These generators are 2-10 times faster than the original implementation, which goes to show how efficient this method is.

## Our Application

We implemented two Ziggurat R functions, one for the Normal distribution and one for the Exponential distribution: rnormzig and rexpzig. Separate functions need to be written for each distribution for two reasons. First, the most effective method for drawing from the tail distribution is unique.  For the normal, Marsaglia recommends
$$
X = -ln(U_1)/r, Y = -ln(U_2) 
\\
U_1,U_2 \sim \mathrm{Uniform}(0,1)
\\
\text{if } 2Y > X^2, 
\\
Z_{tail} = r + X
$$
and for the exponential he recommends
$$
X = r - log(U), U \sim \mathrm{Uniform}(0,1)
$$
Second, the Normal distribution is symmetric about zero, whereas the Exponential distribution is not. After generating a positive Normal x value using the Ziggurat construction, symmetry is enforced by randomly assigning a sign to that value. This step is not necessary when sampling from the Exponential distribution because the distribution is defined only on $[0, infty)$.
 To draw from the rectangles, the code just requires the pdf and the inverse pdf for the distribution. This is done by first computing the breakpoints of each rectangle, which is handled by the function zigtable(). This is done by choosing a random value,$r$, in the domain such that 
$$
v = rf(r) + \int_{r}^{\inf} f(x)dx
$$
$r$ becomes $x_n$ and $x_{n-1}$ is generated iteratively by
$$
x_{i-1} = f^{-1}(\frac{v}{x_i} + f(x_i))
$$
This process repeats until $x_0 = 0$. The challenge here is choosing the optimal $v$ so that the iterative process doesn’t overshoot or undershoot. This is done R’s `uniroot` function
Once the breakpoints are computed, it is easy to choose a random area and draw from the uniform within. First, to choose what rectangle we are sampling from, we randomly sample an integer between 2 and 257 (first index is the tail region) from the discrete uniform distribution using `sample()`. `sample()` is relatively slow compared with C implementations, where the index can be extracted cheaply from a single 32-bit random integer using the last 9 bits. After obtaining the rectangle we are sampling from, we check if the rectangle is the base layer. If the rectangle selected is the base layer then we randomly choose a x-value in the using `runif()` and check if $x_i < x_{i-1}$ or that the x-value is in the fast-accept region. If the x-value is not in the fast-accept region the code switches to the distribution-specific tail generator (defined for exponential and normal above) rather than using the rectangular accept/reject logic. The process is the same if the chosen rectangle is not the bottom layer, but if the x-value is not in the fast-accept region we generate an additional uniform height and accept the point $(x,y)$ only if it lies under the pdf curve.


```{r}
zigtable(function(x) { exp(-x) }, function(y) { -log(y) },8)
```

```{r}
set.seed(777)
x <- rnormzig(1000000)
hist(x, breaks = 100, freq=FALSE)
curve(dnorm(x), add=TRUE, col="red", lwd=2) # looks good
```
```{r testing}
set.seed(777)

#testing: alpha = 0.01 
samples1 <- rnormzig(4999)
print("4999 Samples")
shapiro.test(samples1) # no evidence against normal; p-value > alpha = 0.01
ks.test(samples1, "pnorm", mean=mean(samples1), sd=sd(samples1)) # same

print("1,000,000 ziggurat samples")
# testing 1 mil samples...
ks.test(x, "pnorm", mean=mean(x), sd=sd(x)) # passed
mean(x) # close to 0
sd(x)   # close to 1

print("1,000,000 rnorm samples")
# additional test:
x2 <- rnorm(1000000)
ks.test(x2, "pnorm", mean=mean(x2), sd=sd(x2)) # passed
mean(x2)  # close to 0
sd(x2)    # close to 1

# QQ plot
qqnorm(x, pch = 1, frame = FALSE)
qqline(x, col = "steelblue", lwd = 2) # basically normal
```


Assume we generate 100000 with Ziggurat method and they're actually normal, using rnorm() for now

```{r}
set.seed(77)
x_exp <- rexpzig(1000000)
hist(x_exp, breaks = 100, freq = FALSE)
curve(dexp(x), add=TRUE, col="red", lwd=2) # looks good
```
```{r}
set.seed(777)
qqplot(qexp(ppoints(length(x_exp)), rate = 1/mean(x_exp)),
       sort(x_exp),
       xlab = "Theoretical Exp Quantiles",
       ylab = "Sample Quantiles")
qqline(x_exp, col = "steelblue", lwd = 2) # basically normal

print("1,000,000 Ziggurat samples")
mean(x_exp) #Should be close to 1
sd(x_exp) #Should be close to 1
ks.test(x_exp, "pexp", rate = 1/mean(x_exp)) #large p-value -> fail to reject expoential distribution

x2_exp <- rexp(1000000)
sum(duplicated(x2_exp))
mean(x2_exp) #Should be close to 1
sd(x2_exp) #Should be close to 1
ks.test(x2_exp, "pexp", rate = 1/mean(x2_exp))
```

## Testing For Accuracy

In order to validate that the Ziggurat method is actually generating the random variables of interest, we performed the chi-square goodness of fit test (Knuth 1997). This test is used to check the distribution of the random variables generated by quantizing the horizontal axis of the probability density function into k bins and deriving a single value as a quality metric from the determined actual and expected number of samples in each bin. We constructed an R function based on the chi-square statistic formula: 2k-1 =i=1k(Yi-tpi)2/ tpi.
$$
\chi^2_{k-1} = \sum^k_{i=1}\frac{(Y_i-tp_i)^2} {tp_i}
$$
$t =$ number of observations

$p_i = $probability that each observation falls into the the category i

$Y_i =$ the number of observation that actually do fall into the category i

Using the rnormzig() function to generate 1,000,000 random variates, the distribution generated was tested based on 200 bins spaced uniformly over $[-7, 7]$  which we chose based on the paper that also analyzes the Ziggurat method (Leong et al. 2005). The calculated 2 value was 174.2026 and 184.4201 for the Ziggurat method and the rnorm() function in R, respectively. The critical value for a chi-square test with 199 degrees of freedom at 95% confidence level ($\alpha= 0.05$) is 232.912. Our implementation was also ran over increasing number of samples– one hundred thousand, a million, etc– which all had a chi-square value less than the critical value of 232.912. Thus, we have successfully generated random variables that follow a normal distribution. 


```{r}
chi_squared_test_norm <- function(x, k){
  t <- length(x) # number of observations
  bins <- seq(-7, 7, length.out = k+1) #200 bins spaced uniformly over data edge of min and max
  
  Yi <- hist(x, breaks = bins, plot = FALSE)$counts # observed counts
  # expected probabilites (p_i) of standard normal
  p_i <- numeric(k)
  for (i in 1:k) {
    p_i[i] <- pnorm(bins[i+1]) - pnorm(bins[i])
}
  # using computing chi square using equation from the paper
  chi_sq <- sum((Yi - t * p_i)^2 / (t * p_i))
  df <- k - 1
  return(data.frame(chi_square = chi_sq,
                    df = df))
}





```

```{r}
set.seed(777)

k <- 200
results <- chi_squared_test_norm(x, k)
results2 <- chi_squared_test_norm(x2, k)

results$Method <- "Ziggurat"
results2$Method <- "Rnorm"

combined <- rbind(results, results2)
combined <- combined[, c("Method", setdiff(names(combined), "Method"))]
kable(combined, caption = "Chi-squared value: Ziggurat vs Rnorm")
```

```{r}
n <- c(10000, 100000, 1000000, 10000000)
set.seed(777)
k <- 200 
value <- numeric(length(n))
for(i in 1:length(n)){
  vectors <- rnormzig(n[i])
  chi_sq <- chi_squared_test_norm(vectors, k)
  value[i] <- chi_sq$chi_square 
  
}


df <- data.frame(
  n = n,
  chi_square = value
)

ggplot(df, aes(x = n, y = chi_square)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Number of Samples",
    y = "Chi-square Statistic",
    title = expression(chi^2 ~ "Statistic vs Sample Size")
  ) +
  theme_bw()
```
# Chi-squared Test for Exponential 
```{r}
chi_squared_test_exp <- function(x, k){
  t <- length(x) # number of observations
  bins <- seq(0, max(x), length.out = k+1) #200 bins spaced uniformly over data edge of min and max
  
  Yi <- hist(x, breaks = bins, plot = FALSE)$counts # observed counts
  # expected probabilites (p_i) of standard normal
  p_i <- numeric(k)
  for (i in 1:k) {
    p_i[i] <- pexp(bins[i+1]) - pexp(bins[i])
}
  # using computing chi square using equation from the paper
  chi_sq <- sum((Yi - t * p_i)^2 / (t * p_i))
  df <- k - 1
  return(data.frame(chi_square = chi_sq,
                    df = df))
}
```

```{r}
set.seed(777)

k <- 200
results_exp <- chi_squared_test_exp(x_exp, k)
results2_exp <- chi_squared_test_exp(x2_exp, k)

results_exp$Method <- "Ziggurat"
results2_exp$Method <- "Rexp"

combined <- rbind(results_exp, results2_exp)
combined <- combined[, c("Method", setdiff(names(combined), "Method"))]
kable(combined, caption = "Chi-squared value: Ziggurat vs Rexp")
```
```{r}
set.seed(777)
n <- c(10000, 100000, 1000000, 10000000)
k <- 200 
value <- numeric(length(n))
for(i in 1:length(n)){
  vectors <- rexpzig(n[i])
  chi_sq <- chi_squared_test_exp(vectors, k)
  value[i] <- chi_sq$chi_square 
  
}


df <- data.frame(
  n = n,
  chi_square = value
)

ggplot(df, aes(x = n, y = chi_square)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(
    x = "Number of Samples",
    y = "Chi-square Statistic",
    title = expression(chi^2 ~ "Statistic vs Sample Size")
  ) +
  theme_bw()
```
## Works Cited

